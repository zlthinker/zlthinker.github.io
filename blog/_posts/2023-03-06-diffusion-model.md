---
title: Diffusion Model In Essence
updated: 2023-03-06 15:00
tags: [matrix, math]
category: Linear Algebra
---

<p align="center">
<img src="/images/diffusion_model/diffusion_model_teaser.jpg" alt="diffusion_model_teaser" width="500"/>
</p>
<p align="center">
<span class="footer"> <i> Generated by Diffusion Model </i></span>
</p>

## The Essence of Diffusion Model

The essence of diffusion model learning is to learn a mapping between **an unknown data distribution** and **a known distribution**, i.e., $$\mathcal{T}: R^d \rightarrow R^d$$. The distributions have the same dimensions. With a known distribution, we can draw a sample from it easily and generate infinite samples as we want, and map it back to the unknown distribution. 

Theoretically, we can choose any known distributions we like. But in general, standard Gaussian distribution is used, because it is easy to understand and simple to formulate and implement.

Generally speaking, the dimension of the known distribution is the higher the better, because higher dimension means larger capacity for capturing a complex data distribution. But as the dimension grows higher, it is more difficult to train the mapping and less efficient to run sampling. That is why [StableDiffusion](https://stablediffusionweb.com/) uses a latent space with reduced dimension and learn the mapping in the latent space instead of the high-dimensional pixel space. At the same time, **it has the benefit of compressing the low-level information in images and only keep the essential semantic information in the distribution in the latent space.** Besides, the dimension of the latent space depends on the complexity of specific tasks.




## Probabilistic Modeling

Knowing the mapping from a data distribution of interest (e.g., an image set) to a Gaussian distribution is useless by itself, while the reverse is what we want. The reverse mapping from a sample in Gaussian distribution (e.g., $$y \in N(0,\Sigma)$$) to a sample in an image set is not deterministic, but probabilistic. Given a Gaussian sample $$y$$, learning the reverse mapping is to uncover the conditional probability $$p(x\|y)$$, where $$x$$ is a sample in image set. Mathematically, knowing $$p(x\|y)$$ is equivalent to knowing $$p(x, y)$$, since $$p(x, y) = p(y) p(x\|y)$$ and $$p(y)$$ is already known.


The transform from any distribution to a Gaussian distribution can be easily modelled as a diffusion process in Physics, where noise is added into the data gradually with a self-defined Markov chain. Similarly, we can also learn the reverse process gradually in many small steps, as learning a single-step mapping $$p(x\|y)$$ is almost intractable when the distribution of $$x$$ is complicated.

Once we decompose the mapping into a bunch of small steps, we get the hidden variables in the same number as the steps. Let's denote the variable of the image set distribution as $$x_0$$, and the variable of the final Gaussian distribution as $$x_T$$. We have a number of $$T-1$$ intemediate hidden variables $$x_1, x_2, ..., x_{T-1}$$ transitioning from $$x_0$$ to $$x_T$$. The number of the steps depend on how far away the original distribution of $$x_0$$ is from a Gaussian distribution. 

The benefit of these hidden variables is that they make it easier to learning mapping between two very different distributions. On the one hand, it is more effortless to train small substeps than a single huge step, because the regularization of the hidden variable distributions can make the optimization more stable. On the other hand, if one step fails to learn well, the other steps can compensate for that and give an overall good performance. This is how we get the figure below:

<p align="center">
<img src="/images/diffusion_model/diffusion_model_markov_chain.png" alt="diffusion_model_markov_chain" width="700"/>
</p>




