<!DOCTYPE html>
<html lang="en">

	<head>
		<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1" />


	<title>Caffe 学习</title>


<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@" />
<meta name="twitter:title" content="Caffe 学习" />
<meta name="twitter:description" content="">

<meta name="description" content="">


	<meta name="google-site-verification" content="epFgX0s_0RM3CdjwFcsewfXzPov2g8s9ZBOLyaIUH-o">


<link rel="icon" href="/assets/favicon.png">
<link rel="apple-touch-icon" href="/assets/touch-icon.png">
<link href="https://fonts.googleapis.com/css?family=Karla" rel="stylesheet">
<link rel="stylesheet" href="/assets/core.css">
<link rel="canonical" href="/Caffe">
<link rel="alternate" type="application/atom+xml" title="Lei Zhou's Blog" href="/feed.xml" />




<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


		<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
		<style>
			#taglist {
				color: gray;
				margin: 0;
				margin-top: 16px;
				padding: 0;
				list-style-type: none;
				text-align: center;
				vertical-align: middle;
			}
			#taglist li {
				display: inline;
			}
			#taglist li + li:before {
				content: ", ";
			}
		</style>

		<!-- Google Tag Manager -->
		<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
			new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
			j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
			'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
			})(window,document,'script','dataLayer','GTM-T5S7PZV');</script>
		<!-- End Google Tag Manager -->
	</head>

	<body>
		<!-- Google Tag Manager (noscript) -->
		<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T5S7PZV"
			height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
		<!-- End Google Tag Manager (noscript) -->

		
<aside class="logo">

	

	<a href="/">
		<img src="/images/avatar.jpeg" class="gravatar">
	</a>
	<span class="logo-prompt">Back to Home</span>

</aside>


		<main>
			<noscript>
	<style>
		article .footnotes {
			display: block;
		}
	</style>
</noscript>

<article>
	<div class="center">
		<h1>Caffe 学习</h1>
		<time>January 2, 2017</time>
		<!-- <ul id="taglist">
			Tag:
			
			<li><a href="/tag_index.html" title="Caffe">Caffe</a></li>
			
			<li><a href="/tag_index.html" title="GPU">GPU</a></li>
			
		</ul> -->
	</div>

	<div class="divider"></div>

	<h1 id="源码学习">源码学习</h1>

<h3 id="layersetup-vs-reshape">LayerSetUp VS. Reshape</h3>

<p>每个Caffe的layer都必须定义一个LayerSetUp()函数，用来执行解析layer parameters以及判断bottom blob shape是否正确等初始化操作。而Reshape()函数主要负责管理blob shape，包括设定top blob的shape以及中间blob buffer的shape等等。但在实际的实现中，两个函数的功能的实现偶有重叠。因此，正确认清两者的subtle difference，才能避免错误的产生并合理组织代码的逻辑。</p>

<p>LayerSetUp()函数在layer的生命周期中只在一开始初始化时被执行一次。而Reshape()函数在每次forward的时候都会被调用，从而动态的分配blob的大小。因此，如果top blob和intermediate blob的形状是固定的，Reshape()的代码块可以合并到LayerSetUp()函数中；但是对于convolution或pooling等对blob shape没有固定要求的layer，blob shape的管理必须在Reshape()函数中实现。That’s why it is called <strong>Re</strong>shape().</p>

<h3 id="mutable_cpu_data-vs-cpu_data">mutable_cpu_data VS. cpu_data</h3>

<p>在Caffe的blob类中定义了<code class="language-plaintext highlighter-rouge">mutable_cpu_data()</code>和<code class="language-plaintext highlighter-rouge">cpu_data()</code>两种member function。从它们的定义来看两者的区别其实非常小。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>const void* SyncedMemory::cpu_data() {
	check_device();
	to_cpu();
	return (const void*)cpu_ptr_;
}
void* SyncedMemory::mutable_cpu_data() {
	check_device();
	to_cpu();
	head_ = HEAD_AT_CPU;
	return (const void*)cpu_ptr_;
}
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">cpu_data()</code>返回的是常指针，说明该函数只能用来读取数据。<code class="language-plaintext highlighter-rouge">mutable_cpu_data()</code>函数用来修改数据，并且多了一句<code class="language-plaintext highlighter-rouge">head_ = HEAD_AT_CPU;</code>，用来提醒系统CPU memory中的数据已经被修改过了，需要小心。类似地，<code class="language-plaintext highlighter-rouge">mutable_gpu_data()</code>和<code class="language-plaintext highlighter-rouge">gpu_data()</code>的差别也是如此。</p>

<h3 id="gpu-cuda-and-caffe">GPU, CUDA and Caffe</h3>

<p>GPU的并行架构要从两个方面来理解，一是硬件，二是软件。</p>

<h5 id="硬件">硬件</h5>

<p>GPU硬件运算模块主要由多个(e.g. 16)个<strong>SM (streaming multiprocessor)</strong>组成，也叫GPU大核。每个SM由多个(e.g. 192 for Kepler) <strong>SP (streaming processor)</strong>组成，它是实际执行指令和任务的基本处理单元。因此一个GPU可以包含上千个甚至更多的SP，这正是其强大所在。从这个角度来理解，一个quad-core CPU可以认为是只有一个SM和4个SP的简易版GPU。</p>

<h5 id="软件">软件</h5>

<p>在CUDA编程架构里，有<strong>thread</strong>, <strong>block</strong>, <strong>grip</strong>, <strong>warp</strong>等等概念。</p>

<ul>
  <li><strong>thread</strong>: 执行指令</li>
  <li><strong>block</strong>: 数个(512 for CUDA 8.0)threads组成一个block，同一个block中的threads可以通过shared memory通信。</li>
  <li><strong>grid</strong>: 数个blocks构成grid。</li>
  <li><strong>warp</strong>: GPU执行程序时的<strong>调度单位</strong>，目前CUDA的warp大小为32，同在一个warp的线程执行相同的指令（即相同的kernel function），这就是所谓的SIMT。</li>
</ul>

<h5 id="对应关系">对应关系</h5>

<p>程序任务在最小的粒度上由一个个threads组成，一开始CUDA会将threads组成若干个thread blocks（512 threads per block in CUDA 8.0），然后将这些blocks交由SM来运行（8 active blocks per SM in Fermi），同时每个SM上会给这些thread blocks的所有线程分配资源，如shared memory。因为一个SP只可以运行一个thread，而SM上线程多而SP少，所以需要调度来保证公平、避免starvation。因此，在最新的CUDA架构Fermi中，32个threads被分成一个warp（相当于一个排），以warp作为调度和运行的基本单元。在Fermi中一个SM可以同时运行48个active warps，由warp scheduler负责调度（相当于连长调度所有的排轮流执行任务）。因为一个SM的硬件资源在一开始就已经分配给了blocks和threads，每个thread不管是否正在被执行，都在shared_memory上拥有自己的register等等资源，所以上下文切换不会造成任何开销，这也是GPU比CPU更高效的一个因素。
因为一个SM支持的warp数量以及一个warp包含的线程数量都由具体架构指定，所以在Fermi架构中，一个GPU上的resident thread最多只有SM*48*32=SM*1536个。</p>

<p>要在Caffe layer中使用GPU进行并行计算，可以自定义kernel function来封装要并行的指令。函数的申明前要加<code class="language-plaintext highlighter-rouge">__device__</code>或<code class="language-plaintext highlighter-rouge">__global__</code>关键字。调用该function的方法是
<code class="language-plaintext highlighter-rouge">kernel_function&lt;Dtype&gt;&lt;&lt;&lt;CAFFE_GET_BLOCKS(count), CAFFE_CUDA_NUM_THREADS&gt;&gt;&gt;</code>，
它会根据可并行单元的数量和每个block最多包含的线程数<code class="language-plaintext highlighter-rouge">CAFFE_CUDA_NUM_THREADS </code>来分恰当数量的block。</p>

<h3 id="cuda--opencl">CUDA &amp; OpenCL</h3>

<p>CUDA和OpenCL的关系都和DirectX与OpenGL的关系很相像。如同DirectX和OpenGL一样，CUDA和OpenCL中，前者是配备完整工具包、针对单一供应商(NVIDIA)的成熟的开发平台，后者是一个开放的标准。虽然两者抱着相同的目标：通用并行计算。但是CUDA仅仅能够在NVIDIA的GPU硬件上运行，而OpenCL的目标是面向任何一种Massively Parallel Processor，期望能够对不同种类的硬件给出一个相同的编程模型。</p>

<h5 id="reference">Reference</h5>

<p><a href="http://blog.csdn.net/junparadox/article/details/50540602">http://blog.csdn.net/junparadox/article/details/50540602</a></p>

<p><a href="https://chrischoy.github.io/research/making-caffe-layer/">https://chrischoy.github.io/research/making-caffe-layer/</a></p>

<p><a href="http://on-demand.gputechconf.com/gtc-express/2011/presentations/cuda_webinars_WarpsAndOccupancy.pdf">http://on-demand.gputechconf.com/gtc-express/2011/presentations/cuda_webinars_WarpsAndOccupancy.pdf</a></p>

<h1 id="install-caffe-on-centos">Install Caffe on CentOS</h1>

<h3 id="preliminary">Preliminary</h3>
<ul>
  <li>CentOS 7, x86</li>
  <li>Nvidia Geforce 980</li>
  <li>Cuda 8.0</li>
</ul>

<h3 id="problems">Problems</h3>

<h4 id="cublas_v2h-no-such-file-or-directory--compilation-terminated">cublas_v2.h: No such file or directory.  Compilation terminated.</h4>

<blockquote>
  <p>The error is caused by the failure to link to ablas. So install openblas instead:</p>
</blockquote>

<blockquote>
  <p>sudo yum install openblas-devel.x86_64</p>
</blockquote>

<h4 id="cmake-cannot-detect-cuda">cmake cannot detect cuda</h4>

<blockquote>
  <p>Need to add cuda include and library path to global path</p>
</blockquote>

<h3 id="compilation">Compilation</h3>
<ol>
  <li>mkdir build</li>
  <li>cd build</li>
  <li>sudo su</li>
  <li>cmake -DBLAS=open ..</li>
  <li>make -j8</li>
  <li>make install</li>
</ol>


	<div class="divider"></div>

	<!-- LikeBtn.com BEGIN -->
	<div class="footer">
		<span class="block">&hearts; <i> I appreciate your feedback! &hearts; </i></span>
	</div>
	<div style="text-align: center;">
		<span class="likebtn-wrapper" data-identifier="Caffe 学习"></span>
		<script>(function (d, e, s) { if (d.getElementById("likebtn_wjs")) return; a = d.createElement(e); m = d.getElementsByTagName(e)[0]; a.async = 1; a.id = "likebtn_wjs"; a.src = s; m.parentNode.insertBefore(a, m) })(document, "script", "//w.likebtn.com/js/w/widget.js");</script>
	</div>
	<!-- LikeBtn.com END -->

</article>





<div class="page-navigation">
	
	<a class="next" href="/Linux-commands" title="NEXT: Linux 学习">&lt;&lt;</a>
	<span> &middot; </span>
	
	<a class="home" href="/" title="Back to Homepage">Home</a>
	
	<span> &middot; </span>
	<a class="prev" href="/graph-match" title="PREV: Get to Know Graph Matching">&gt;&gt;</a>
	
</div>
		</main>
                                                      

		<div class="footer">
<!--  <span class="block">Made with &hearts; using <a href="http://jekyllrb.com/">Jekyll</a> &amp; <a href="https://github.com/heiswayi/the-plain" title="Lei's Blog">The Plain</a> &middot; &lt;/&gt; on <a href="https://github.com/zlthinker" title="Hosted on GitHub">GitHub</a></span> -->
  <span class="block">
    &copy; 
    2024 
    Lei ZHOU
    <img src="images/dog.jpg" width="35">
  </span>
</div>


		

			

		

		<!-- hit counter -->
		<br>
		<center>
		<!-- <img src="http://hitwebcounter.com/counter/counter.php?page=6818827&style=0024&nbdigits=5&type=ip&initCount=0" align="top" title="" Alt=""   border="1" height=18px/>     -->
		<script type="text/javascript" src="//counter.websiteout.net/js/5/7/10500/1"></script>
		</center>  

	</body>

</html>
