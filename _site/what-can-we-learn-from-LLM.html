<!DOCTYPE html>
<html lang="en">

	<head>
		<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1" />


	<title>What Can We Learn from LLM? Self-Supervised Learning</title>


<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@" />
<meta name="twitter:title" content="What Can We Learn from LLM? Self-Supervised Learning" />
<meta name="twitter:description" content="">

<meta name="description" content="">


	<meta name="google-site-verification" content="epFgX0s_0RM3CdjwFcsewfXzPov2g8s9ZBOLyaIUH-o">


<link rel="icon" href="/assets/favicon.png">
<link rel="apple-touch-icon" href="/assets/touch-icon.png">
<link href="https://fonts.googleapis.com/css?family=Karla" rel="stylesheet">
<link rel="stylesheet" href="/assets/core.css">
<link rel="canonical" href="/what-can-we-learn-from-LLM">
<link rel="alternate" type="application/atom+xml" title="Lei Zhou's Blog" href="/feed.xml" />




<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


		<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
		<style>
			#taglist {
				color: gray;
				margin: 0;
				margin-top: 16px;
				padding: 0;
				list-style-type: none;
				text-align: center;
				vertical-align: middle;
			}
			#taglist li {
				display: inline;
			}
			#taglist li + li:before {
				content: ", ";
			}
		</style>

		<!-- Google Tag Manager -->
		<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
			new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
			j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
			'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
			})(window,document,'script','dataLayer','GTM-T5S7PZV');</script>
		<!-- End Google Tag Manager -->
	</head>

	<body>
		<!-- Google Tag Manager (noscript) -->
		<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T5S7PZV"
			height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
		<!-- End Google Tag Manager (noscript) -->

		
<aside class="logo">

	

	<a href="/">
		<img src="/images/avatar.jpeg" class="gravatar">
	</a>
	<span class="logo-prompt">Back to Home</span>

</aside>


		<main>
			<noscript>
	<style>
		article .footnotes {
			display: block;
		}
	</style>
</noscript>

<article>
	<div class="center">
		<h1>What Can We Learn from LLM? Self-Supervised Learning</h1>
		<time>June 6, 2024</time>
		<!-- <ul id="taglist">
			Tag:
			
		</ul> -->
	</div>

	<div class="divider"></div>

	<p>Self-supervised learning (SSL) has been the key to the success of LLM. It benefits most from seas of Internet data as it uses data as it is without needing annotations and labels. Particularly when the training data is scaled up, the power of transformers is unleashed, according to <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">scaling law</a>. This is the terrian where only self-supervised learning can do, connecting the two beasts of data and weights. (Supervised transformer sounds like a fallacy to me.)</p>

<p align="center">
<img src="/images/SSL-data-network.jpg" alt="ssl-data-network" style="border-radius:15px; width: 350px;" />
</p>
<p align="center">
<span class="footer"> <i> Self-supervised learning is the best way of combining massive data and huge network so far. </i></span>
</p>

<p>The major outcome of self-supervised learning is to give powerful and meaningful latent representations for raw tokens, just by learning from the distribution of the massive data. Knowing the distribution of a data set is equivalent to knowing the correlations (joint probability) of its elements inside. This is why attention machanism can do a good job in this realm (Attention is correlation IMO). And when the data collection is large enough, it contains sufficient combinations of tokens for the hungry transformer network to learn from it.</p>

<p>Learning a good latent space is so important that we often call an encoder/auto-encoder foundation model if it is well trained to enable the downstream tasks such as decoding, diffusion or multi-modality alignment. There are two approaches that have proven successful: contrastive learning and generative learning. Contrastive learning needs to construct similar and dissimilar pairs from data samples. Although being prone to collapse and instablility issues, it is commonly used for multimodal alignment (such as CLIP) and translation tasks, where data naturally form pairs.</p>

<p>Personally, generative learning is more attractive to me than constrastive ones as it works in a way just like people thinking. If a network is able to generate meaningful things within or even without context as we do, I would achknowledge it has the intelligence as we have. The learning scheme is usually achieved by pre-text tasks, among which I like inpainting most.</p>

<p>Predicting a masked word in a sentence as defined by <a href="https://research.google/pubs/bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding/">BERT</a> is same as the cloze test I did in high school English exams. Visually, imagining missing parts of a scene is a task our brain is excel at. The pretraining job of completing an image from a masked input has been applied to single-view images (<a href="https://arxiv.org/pdf/2111.06377">MAE</a>), multi-view images (<a href="https://arxiv.org/pdf/2210.10716">CroCo</a>) and videos (<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/416f9cb3276121c42eebb86352a4354a-Paper-Conference.pdf">VideoMAE</a>), which enables the mono-vision, 3D vision and temporal vision tasks, respectively. It is still surprising to me that the networks are able to fill in meaningful visual content even more than 80% of the images are missing (e.g., MAE and CroCo).</p>

<p align="center">
<img src="/images/SSL-inpainting.jpg" alt="ssl-inpainting" style="border-radius: 5%; width: 300px;" />
</p>
<p align="center">
<span class="footer"> <i> Inpainting capability is where intelligence emerges. </i></span>
</p>

<p>In conclusion, the scale of data will continue to grow, so are the network parameters. Self-supervised learning using pre-text tasks such as inpainting is the only way of making both parties work together. This lesson has already been learned by image and video diffusion model pretraining. But itâ€™s just the beginning. Computer vison ecompasses many more tasks beyond diffusion, and the same practices should be transferred to all the other tasks, to achieve optimal success.</p>



	<div class="divider"></div>

	<!-- LikeBtn.com BEGIN -->
	<div class="footer">
		<span class="block">&hearts; <i> I appreciate your feedback! &hearts; </i></span>
	</div>
	<div style="text-align: center;">
		<span class="likebtn-wrapper" data-identifier="What Can We Learn from LLM? Self-Supervised Learning"></span>
		<script>(function (d, e, s) { if (d.getElementById("likebtn_wjs")) return; a = d.createElement(e); m = d.getElementsByTagName(e)[0]; a.async = 1; a.id = "likebtn_wjs"; a.src = s; m.parentNode.insertBefore(a, m) })(document, "script", "//w.likebtn.com/js/w/widget.js");</script>
	</div>
	<!-- LikeBtn.com END -->

</article>





<div class="page-navigation">
	
	<a class="home" href="/" title="Back to Homepage">Home</a>
	
	<span> &middot; </span>
	<a class="prev" href="/photography" title="PREV: Photography">&gt;&gt;</a>
	
</div>
		</main>
                                                      

		<div class="footer">
<!--  <span class="block">Made with &hearts; using <a href="http://jekyllrb.com/">Jekyll</a> &amp; <a href="https://github.com/heiswayi/the-plain" title="Lei's Blog">The Plain</a> &middot; &lt;/&gt; on <a href="https://github.com/zlthinker" title="Hosted on GitHub">GitHub</a></span> -->
  <span class="block">
    &copy; 
    2024 
    Lei ZHOU
    <img src="images/dog.jpg" width="35">
  </span>
</div>


		

			

		

		<!-- hit counter -->
		<br>
		<center>
		<!-- <img src="http://hitwebcounter.com/counter/counter.php?page=6818827&style=0024&nbdigits=5&type=ip&initCount=0" align="top" title="" Alt=""   border="1" height=18px/>     -->
		<script type="text/javascript" src="//counter.websiteout.net/js/5/7/10500/1"></script>
		</center>  

	</body>

</html>
