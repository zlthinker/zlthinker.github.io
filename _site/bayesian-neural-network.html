<!DOCTYPE html>
<html lang="en">

	<head>
		<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1" />


	<title>Bayesian Neural Network</title>


<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@" />
<meta name="twitter:title" content="Bayesian Neural Network" />
<meta name="twitter:description" content="">

<meta name="description" content="">


	<meta name="google-site-verification" content="epFgX0s_0RM3CdjwFcsewfXzPov2g8s9ZBOLyaIUH-o">


<link rel="icon" href="/assets/favicon.png">
<link rel="apple-touch-icon" href="/assets/touch-icon.png">
<link href="https://fonts.googleapis.com/css?family=Karla" rel="stylesheet">
<link rel="stylesheet" href="/assets/core.css">
<link rel="canonical" href="/bayesian-neural-network">
<link rel="alternate" type="application/atom+xml" title="Lei Zhou's Blog" href="/feed.xml" />




<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


		<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
		<style>
			#taglist {
				color: gray;
				margin: 0;
				margin-top: 16px;
				padding: 0;
				list-style-type: none;
				text-align: center;
				vertical-align: middle;
			}
			#taglist li {
				display: inline;
			}
			#taglist li + li:before {
				content: ", ";
			}
		</style>

		<!-- Google Tag Manager -->
		<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
			new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
			j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
			'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
			})(window,document,'script','dataLayer','GTM-T5S7PZV');</script>
		<!-- End Google Tag Manager -->
	</head>

	<body>
		<!-- Google Tag Manager (noscript) -->
		<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T5S7PZV"
			height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
		<!-- End Google Tag Manager (noscript) -->

		
<aside class="logo">

	

	<a href="/">
		<img src="/images/avatar.jpeg" class="gravatar">
	</a>
	<span class="logo-prompt">Back to Home</span>

</aside>


		<main>
			<noscript>
	<style>
		article .footnotes {
			display: block;
		}
	</style>
</noscript>

<article>
	<div class="center">
		<h1>Bayesian Neural Network</h1>
		<time>May 14, 2018</time>
		<!-- <ul id="taglist">
			Tag:
			
		</ul> -->
	</div>

	<div class="divider"></div>

	<ul id="markdown-toc">
  <li><a href="#related-works" id="markdown-toc-related-works">Related Works</a></li>
  <li><a href="#formulation" id="markdown-toc-formulation">Formulation</a></li>
  <li><a href="#optimization" id="markdown-toc-optimization">Optimization</a></li>
  <li><a href="#dropout-as-variational-inference" id="markdown-toc-dropout-as-variational-inference">Dropout as Variational Inference</a></li>
</ul>

<h3 id="related-works">Related Works</h3>

<ul>
  <li>[1] <a href="https://www.coursera.org/learn/bayesian-methods-in-machine-learning/lecture/HI8ta/bayesian-neural-networks">Coursera: Bayesian Neural Networks</a></li>
  <li>[2] SGLD: <a href="https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf">Bayesian Learning via Stochastic Gradient Langevin Dynamics</a>: (ICML2011)</li>
  <li>[3] preconditioned SGLD: <a href="http://people.ee.duke.edu/~lcarin/aaai_psgld_final.pdf">Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks</a>: (AAAI 2016)</li>
  <li>[4] <a href="https://arxiv.org/pdf/1506.04416.pdf">Bayesian Dark Knowledge</a>: (NIPS2015)</li>
  <li>[5] <a href="https://arxiv.org/pdf/1509.05909.pdf">Modelling Uncertainty in Deep Learning for Camera Relocalization</a></li>
  <li>[6] <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Brachmann_DSAC_-_Differentiable_CVPR_2017_paper.pdf">DSAC-Differentiable RANSAC for camera localization</a></li>
  <li>
    <p>[7] <a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Learning_Weight_Uncertainty_CVPR_2016_paper.pdf">Learning Weight Uncertainty with Stochastic Gradient MCMC for Shape Classification</a></p>
  </li>
  <li>Dropout as Bayesian Neural Network</li>
  <li>[8] <a href="https://arxiv.org/pdf/1506.02142.pdf">Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</a>: ICML, 2016</li>
  <li>[9] <a href="https://arxiv.org/pdf/1506.02158.pdf">Bayesian convolutional neuralnetworks with Bernoulli approximate variational inference</a>: ICLR workshop, 2016</li>
  <li>[10] <a href="https://arxiv.org/pdf/1601.00670.pdf">Variational Inference: A Review for Statisticians</a></li>
</ul>

<h3 id="formulation">Formulation</h3>

<p>Given a training dataset \(\mathcal{D} = \{d_i\}_{i=1}^N = \{(x_i, y_i)\}_{i=1}^N\) where \(d_i = (x_i, y_i)\) is a pair of input datum and label, the objective of training is to find the optimal model parameter \(\mathbf{\theta}\) which maximizes the posterior \(p(\theta \| \mathcal{D})\), i.e.,</p>

\[\theta^* = \arg\max_{\theta} p(\theta \| \mathcal{D}).\]

<p>The posterior can be expressed as \(p(\theta \| \mathcal{D}) \propto p(\theta) p(\mathcal{D} \| \theta)\) (as known, prior \(\times\) likelihood). Taking the logarithm of both sides, we get \(\log p(\theta \| \mathcal{D}) = \log p(\theta) + \log p(\mathcal{D} \| \theta) + Const\). The first term of RHS, related to the prior distribution, is actually the regularization loss of model weights used in optimization. The second term is equal to the defined loss measuring how well the learned parameter \(\theta\) fits the training dataset \(\mathcal{D}\).</p>

<p>Given a new testing example \(x\), the prediction of its output \(y\) is given by</p>

\[p(y \| x, \mathcal{D}) = \int_{\theta} p(\theta \| \mathcal{D}) p(y \| x, \theta) d\theta = E_{p(\theta \| \mathcal{D})}[p(y \| x, \theta)].\]

<p>Intuitively, it takes the average over the distribution of learned model parameters \(\theta\). 
Traditionally, the prediction of \(y\) is given by \(f(\theta^*, x)\), where \(y^*\) is deterministically determined by \(\theta^*\). If we are aware of the distribution of \(p(\theta \| \mathcal{D})\), the prediction can be averaged as</p>

\[E(y\|x, D) = \int_{\theta} p(\theta \| \mathcal{D}) f(\theta, x) d\theta = E_{p(\theta \| \mathcal{D})}[f(\theta, x)].\]

<p>Exact estimation of the distribution \(p(\theta \| \mathcal{D})\) is intractable. A feasible solution is to apply Monte Carlo sampling to sample \(\{\theta_i \}_{i=1}^m\) from the latent distribution \(p(\theta \| \mathcal{D})\). Therefore the estimation of a new testing example is approximated as</p>

\[p(y \| x, \mathcal{D}) = \frac{1}{m} \sum_{i=1}^m p(y \| x, \theta_i).\]

<p>In this way, MAP can be regarded as an over-confident approximation of the equation above which assumes</p>

\[p(\theta \| \mathcal{D} ) = \begin{cases} 1,  \theta = \theta^* \\ 0 ,  \theta \neq \theta^* \end{cases}.\]

<h3 id="optimization">Optimization</h3>

<p>The update equation of Gradient Descent (GD) at iteration \(t\) is expressed as</p>

\[\triangle \theta_t = \frac{\epsilon_t}{2} \left( \nabla \log p(\theta_t) + \sum_{i=1}^N \nabla \log p(d_i \| \theta_t) \right),\]

<p>where \(\epsilon_t\) is the learning rate or step size at iteration \(t\) and we assume the training data \(\{d_i \}\) follow iid.</p>

<p>In Stochastic Gradient Descent (SGD), the statistics of the whole dataset is approximated by a mini-batch. The update equation is written into</p>

\[\triangle \theta_t = \frac{\epsilon_t}{2} \left( \nabla \log p(\theta_t) + \frac{N}{n} \sum_{i=1}^n \nabla \log p(d_{ti} \| \theta_t) \right)\]

<p>by considering a randomly-selected mini-batch \(\{d_{ti} \}_{i=1}^n\).</p>

<h3 id="dropout-as-variational-inference">Dropout as Variational Inference</h3>

<p>The main rule of <strong>variational infernece</strong> is to approximate the posterior distribution \(p(\theta \| \mathcal{D})\) by a defined variational distribution \(q(\theta)\) which is easy to evaluate. The error of approximation is measured by the KL diverence:</p>

\[KL(q(\theta) , p(\theta \| \mathcal{D}))\]

<p>As stated in [10], minimize the KL divergence above is equivalent to minimize the upper bound:</p>

\[\begin{split} -E_{q(\theta)}[\log p(\mathcal{D} \| \theta)] + KL(q(\theta) , p(\theta) ) &amp;= - \int q(\theta)\log p(\mathcal{D} \| \theta) d\theta + KL(q(\theta) , p(\theta) ) \\ &amp;= -\sum_{n=1}^N \int q(\theta)\log p(d_i \| \theta) d\theta  + KL(q(\theta) , p(\theta) ) \end{split}.\]

<p>In [8][9], the dropout is applied after each convolutional layer to simulate the variational distribution \(q(\theta)\). 
If we assume \(q(\theta)\) follows certain Gaussian Process (GP), “Dropout in NNs can be interpreted as an approximation to a well know Bayesian model - the Gaussian Process (GP)”, as said in [9].
The parameters \(\theta\) is composed of a set of parameters of the convolutional layers, i.e., \(\theta = \{ W_i, b_i \}_{i=1}^L\). \(W_i\) is a \(K_i \times K_{i-1}\) matrix involved in the mapping from the output of the layer \(i-1\) to the output of layer \(i\), i.e., \(\sigma(W_i \cdot o_{i-1} + b_i)\). Once dropout is applied, the components of the output of the layer \(i-1\) are ramdoms set to zero with probability \(p_i\). Therefore, the distribution of \(W_i\) follows</p>

\[W_i = M_i \cdot diag([z_{i,j}]_{j=1}^{K_{i-1}}),\]

\[z_{i,j} \sim Bernoulli(p_i).\]

<p>Herein \(M_i\) and \(b_i\)are variational parameters to be inferred. In this way, the variational distribution \(q(\theta)\) is defined through dropout. Then the integeral \(\int q(\theta)\log p(d_i \| \theta) d\theta\) is approximated by Monte Carlo with a single sample \(\theta_n \sim q(\theta)\) to get an unbiased estimate \(\log p(d_i \| \theta_n)\).</p>

<p>In [8], the term \(KL(q(\theta) , p(\theta) )\) is approximated by \(\lambda \sum_{i=1}^L (\|W_i \|_2^2 + \|b_i \|_2^2)\). Therefore, the error of KL divergence is written into</p>

\[KL(q(\theta) , p(\theta \| \mathcal{D})) = -\sum_{n=1}^N \log p(d_i \| \theta) + \lambda \sum_{i=1}^L (\|W_i \|_2^2 + \|b_i \|_2^2),\]

<p>which actually has the equivallent objective of learning Dropout Neural Networks. In other words, inferring the posterior distribution of \(p(\theta \| \mathcal{D})\) via a Bayesian Neural Network is equivalent to learning variational parameters \(M_i\) and \(b_i\) via a Dropout Neural Network.</p>



	<div class="divider"></div>

	<!-- LikeBtn.com BEGIN -->
	<div class="footer">
		<span class="block">&hearts; <i> I appreciate your feedback! &hearts; </i></span>
	</div>
	<div style="text-align: center;">
		<span class="likebtn-wrapper" data-identifier="Bayesian Neural Network"></span>
		<script>(function (d, e, s) { if (d.getElementById("likebtn_wjs")) return; a = d.createElement(e); m = d.getElementsByTagName(e)[0]; a.async = 1; a.id = "likebtn_wjs"; a.src = s; m.parentNode.insertBefore(a, m) })(document, "script", "//w.likebtn.com/js/w/widget.js");</script>
	</div>
	<!-- LikeBtn.com END -->

</article>





<div class="page-navigation">
	
	<a class="next" href="/mean-field-approximation" title="NEXT: Mean Field Approximation">&lt;&lt;</a>
	<span> &middot; </span>
	
	<a class="home" href="/" title="Back to Homepage">Home</a>
	
	<span> &middot; </span>
	<a class="prev" href="/optimization-for-least-square-problem" title="PREV: Optimization for Least Square Problems">&gt;&gt;</a>
	
</div>
		</main>
                                                      

		<div class="footer">
<!--  <span class="block">Made with &hearts; using <a href="http://jekyllrb.com/">Jekyll</a> &amp; <a href="https://github.com/heiswayi/the-plain" title="Lei's Blog">The Plain</a> &middot; &lt;/&gt; on <a href="https://github.com/zlthinker" title="Hosted on GitHub">GitHub</a></span> -->
  <span class="block">
    &copy; 
    2025 
    Lei ZHOU
    <img src="images/dog.jpg" width="35">
  </span>
</div>


		

			

		

		<!-- hit counter -->
		<br>
		<center>
		<!-- <img src="http://hitwebcounter.com/counter/counter.php?page=6818827&style=0024&nbdigits=5&type=ip&initCount=0" align="top" title="" Alt=""   border="1" height=18px/>     -->
		<script type="text/javascript" src="//counter.websiteout.net/js/5/7/10500/1"></script>
		</center>  

	</body>

</html>
