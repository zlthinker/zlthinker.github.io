<!DOCTYPE html>
<html lang="en">

	<head>
		<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1" />


	<title>Image Localization</title>


<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@" />
<meta name="twitter:title" content="Image Localization" />
<meta name="twitter:description" content="">

<meta name="description" content="">


	<meta name="google-site-verification" content="epFgX0s_0RM3CdjwFcsewfXzPov2g8s9ZBOLyaIUH-o">


<link rel="icon" href="/assets/favicon.png">
<link rel="apple-touch-icon" href="/assets/touch-icon.png">
<link href="https://fonts.googleapis.com/css?family=Karla" rel="stylesheet">
<link rel="stylesheet" href="/assets/core.css">
<link rel="canonical" href="/image-localization">
<link rel="alternate" type="application/atom+xml" title="Lei Zhou's Blog" href="/feed.xml" />




<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


		<link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
		<style>
			#taglist {
				color: gray;
				margin: 0;
				margin-top: 16px;
				padding: 0;
				list-style-type: none;
				text-align: center;
				vertical-align: middle;
			}
			#taglist li {
				display: inline;
			}
			#taglist li + li:before {
				content: ", ";
			}
		</style>

		<!-- Google Tag Manager -->
		<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
			new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
			j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
			'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
			})(window,document,'script','dataLayer','GTM-T5S7PZV');</script>
		<!-- End Google Tag Manager -->
	</head>

	<body>
		<!-- Google Tag Manager (noscript) -->
		<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T5S7PZV"
			height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
		<!-- End Google Tag Manager (noscript) -->

		
<aside class="logo">

	

	<a href="/">
		<img src="/images/avatar.jpeg" class="gravatar">
	</a>
	<span class="logo-prompt">Back to Home</span>

</aside>


		<main>
			<noscript>
	<style>
		article .footnotes {
			display: block;
		}
	</style>
</noscript>

<article>
	<div class="center">
		<h1>Image Localization</h1>
		<time>March 16, 2018</time>
		<!-- <ul id="taglist">
			Tag:
			
		</ul> -->
	</div>

	<div class="divider"></div>

	<ul id="markdown-toc">
  <li><a href="#related-works" id="markdown-toc-related-works">Related works</a>    <ul>
      <li><a href="#iccv2019" id="markdown-toc-iccv2019">ICCV2019</a></li>
      <li><a href="#works-before-iccv2019" id="markdown-toc-works-before-iccv2019">Works before ICCV2019</a></li>
    </ul>
  </li>
  <li><a href="#pnp-algorithm" id="markdown-toc-pnp-algorithm">PnP algorithm</a>    <ul>
      <li><a href="#p3p" id="markdown-toc-p3p">P3P</a></li>
      <li><a href="#p4p-p5p-pnp" id="markdown-toc-p4p-p5p-pnp">P4P, P5P, PnP</a></li>
      <li><a href="#epnp" id="markdown-toc-epnp">EPnP</a></li>
      <li><a href="#other-pnps-and-insights" id="markdown-toc-other-pnps-and-insights">Other PnPs and insights</a></li>
    </ul>
  </li>
  <li><a href="#scene-coordinate-regression-network-score-net" id="markdown-toc-scene-coordinate-regression-network-score-net">Scene COordinate REgression Network (SCORE-Net)</a>    <ul>
      <li><a href="#network-architecture" id="markdown-toc-network-architecture">Network architecture</a></li>
    </ul>
  </li>
  <li><a href="#reference" id="markdown-toc-reference">Reference</a></li>
</ul>

<h1 id="related-works">Related works</h1>

<h2 id="iccv2019">ICCV2019</h2>
<ul>
  <li>
    <p><a href="https://arxiv.org/pdf/1908.06387.pdf">Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization</a> segmentation to handler season changes</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1908.06141.pdf">Cascaded Parallel Filtering for Memory-Efficient Image-Based Localization</a> filtering for matching against SfM</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1808.08779.pdf">Stochastic Attraction-Repulsion Embedding for Large Scale Image Localization</a> image retrieval</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1908.04598.pdf">Is This The Right Place? Geometric-Semantic Pose Verification for Indoor Visual Localization</a> pose verification (e.g. used in ransac), needs semantic info</p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1908.04391.pdf">Local Supports Global: Deep Camera Relocalization with Sequence Enhancement</a></p>
  </li>
  <li>
    <p><a href="http://vladlen.info/papers/does-vision-matter.pdf">Does Computer Vision Matter for Action?</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1908.02484.pdf">Expert Sample Consensus Applied to Camera Re-Localization</a></p>
  </li>
  <li>
    <p><a href="">	CamNet: Coarse-to-Fine Retrieval for Camera Re-Localization</a></p>
  </li>
  <li>
    <p><a href="">Prior Guided Dropout for Robust Visual Localization in Dynamic Environments</a></p>
  </li>
</ul>

<h2 id="works-before-iccv2019">Works before ICCV2019</h2>

<ul>
  <li><a href="https://arxiv.org/pdf/1905.04132.pdf">Neural-Guided RANSAC: Learning Where to Sample Model Hypotheses</a></li>
  <li>
    <p><a href="https://arxiv.org/pdf/1804.08366.pdf">VLocNet++</a></p>
  </li>
  <li>Feature-Based
    <ul>
      <li><a href="https://arxiv.org/pdf/1812.03506.pdf">From Coarse to Fine: Robust Hierarchical Localization at Large Scale</a></li>
    </ul>
  </li>
  <li>Tutorial
    <ul>
      <li><a href="https://alexgkendall.com/media/presentations/lsvpr_2017_cvpr_tutorial_alex_kendall.pdf">Learning-based Visual Localization</a></li>
    </ul>
  </li>
  <li>Scene coordinate regression
    <ul>
      <li><a href="http://openaccess.thecvf.com/content_cvpr_2013/papers/Shotton_Scene_Coordinate_Regression_2013_CVPR_paper.pdf">Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images</a></li>
      <li><a href="https://arxiv.org/pdf/1710.07965">Backtracking regression forests for accurate camera relocalization</a></li>
      <li><a href="http://cvlab-dresden.de/HTML/people/alexander_krull/publications/ICRA_2017.pdf">Random forests versus Neural Networks—What’s best for camera localization?</a></li>
      <li><strong>DSAC</strong> <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Brachmann_DSAC_-_Differentiable_CVPR_2017_paper.pdf">DSAC-Differentiable RANSAC for camera localization</a></li>
      <li><strong>DSAC++</strong> <a href="https://arxiv.org/pdf/1711.10228.pdf">Learning Less is More – 6D Camera Localization via 3D Surface Regression</a></li>
      <li><strong>DSAC++ angle loss</strong> <a href="https://arxiv.org/pdf/1808.04999.pdf">Scene Coordinate Regression with Angle-Based Reprojection Loss for Camera Relocalization</a></li>
      <li><strong>Dense regression:</strong> <a href="https://arxiv.org/pdf/1802.03237.pdf">Full-Frame Scene Coordinate Regression for Image-Based Localization</a>: The same idea as mine</li>
      <li><a href="https://arxiv.org/pdf/1805.08443.pdf">Scene Coordinate and Correspondence Learning for Image-Based Localization</a></li>
    </ul>
  </li>
  <li>Pose regression
    <ul>
      <li><a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kendall_PoseNet_A_Convolutional_ICCV_2015_paper.pdf?utm_source=mandiner&amp;utm_medium=link&amp;utm_campaign=mandiner_digit_201512">Posenet: A convolutional network for real-time 6-dof camera relocalization</a></li>
      <li><a href="https://arxiv.org/pdf/1509.05909.pdf">Modelling Uncertainty in Deep Learning for Camera Relocalization</a></li>
      <li><a href="https://arxiv.org/pdf/1611.07890.pdf">Image-based localization using LSTMs for structured feature correlation</a></li>
      <li><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Kendall_Geometric_Loss_Functions_CVPR_2017_paper.pdf">Geometric loss functions for camera pose regression with deep learning</a></li>
      <li>
        <p><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Vassileios_Balntas_RelocNet_Continous_Metric_ECCV_2018_paper.pdf">RelocNet: Continuous Metric Learning Relocalisation using Neural Nets</a>: Retrieval + relative pose regression</p>
      </li>
      <li><a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w17/Laskar_Camera_Relocalization_by_ICCV_2017_paper.pdf">Camera Relocalization by Computing Pairwise Relative Poses Using Convolutional Neural Network</a></li>
      <li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Brahmbhatt_Geometry-Aware_Learning_of_CVPR_2018_paper.pdf">MapNet: Geometry-Aware Learning of Maps for Camera Localization</a></li>
      <li><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Vassileios_Balntas_RelocNet_Continous_Metric_ECCV_2018_paper.pdf">RelocNet: Continuous Metric Learning Relocalisation using Neural Nets</a></li>
    </ul>
  </li>
  <li>Temporal/RNN
    <ul>
      <li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8462979">Deep Auxiliary Learning for Visual Localization and Odometry</a>
        <ul>
          <li><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Clark_VidLoc_A_Deep_CVPR_2017_paper.pdf">VidLoc: A deep spatio-temporal model for 6-DoF video-clip relocalization</a></li>
          <li><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_Deep_Feature_Flow_CVPR_2017_paper.pdf">Deep Feature Flow for Video Recognition</a></li>
          <li><a href="https://arxiv.org/pdf/1606.00487.pdf">Recurrent Fully Convolutional Networks for Video Segmentation</a></li>
          <li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">LSTM</a></li>
          <li><a href="https://arxiv.org/pdf/1711.01124.pdf">End-to-end Flow Correlation Tracking with Spatial-temporal Attention</a> Aggregate temporal feature maps through FlowNet &amp; similarity-weighting.</li>
          <li><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Wei-Sheng_Lai_Real-Time_Blind_Video_ECCV_2018_paper.pdf">Learning Blind Video Temporal Consistency</a> Fuse last prediction and current prediction by directly regressing the residual</li>
          <li><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Tae_Hyun_Kim_Spatio-temporal_Transformer_Network_ECCV_2018_paper.pdf">Spatio-Temporal Transformer Network for Video Restoration</a> Spatio-temporal transformer samples an image from multiple frames (\(T\times H \times W \rightarrow 1\times H \times W\))</li>
          <li><a href="https://arxiv.org/pdf/1804.08758.pdf">Switchable Temporal Propagation Networ</a></li>
        </ul>
      </li>
      <li><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Lu__Online_Video_ICCV_2017_paper.pdf">Online Video Object Detection using Association LSTM</a></li>
      <li><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf">Long-term Recurrent Convolutional Networks for Visual Recognition and Description</a></li>
      <li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Nilsson_Semantic_Video_Segmentation_CVPR_2018_paper.pdf">Semantic Video Segmentation by Gated Recurrent Flow Propagation</a></li>
    </ul>
  </li>
  <li>Traditional
    <ul>
      <li><a href="http://ieeexplore.ieee.org/abstract/document/7572201/">Efficient &amp; effective prioritized matching for large-scale image-based localization</a></li>
    </ul>
  </li>
  <li>Feature selection
    <ul>
      <li><a href="http://www.di.ens.fr/~josef/publications/knopp10.pdf">Avoiding confusing features in place recognition</a>: (ECCV 2010)</li>
      <li>feature re-weighting: <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8099829">Learned Contextual Feature Reweighting for Image Geo-Localization</a></li>
    </ul>
  </li>
  <li>Recent works
    <ul>
      <li><a href="https://arxiv.org/pdf/1712.03342.pdf">MapNet: Geometry-Aware Learning of Maps for Camera Localization</a>: Leverage the full map (CVPR2018, spotlight)</li>
      <li><a href="https://arxiv.org/pdf/1709.09905.pdf">X-View: Graph-Based Semantic Multi-View Localization</a>: Use segmentation</li>
      <li><a href="https://arxiv.org/pdf/1712.05773.pdf">Semantic Visual Localization</a>: Use segmentation (CVPR2018, ETH)</li>
      <li><a href="https://arxiv.org/pdf/1707.09092.pdf">Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions</a>: Outdoor benchmark (CVPR2018, spotlight, ETH)</li>
      <li><a href="">InLoc: Indoor Visual Localization with Dense Matching and View Synthesis</a>: (CVPR2018, spotlight, ETH)</li>
      <li>Pose regression + visual odometry <a href="https://arxiv.org/pdf/1803.03642.pdf">Deep Auxiliary Learning for Visual Localization and Odometry</a>: (ICLA2018)</li>
    </ul>
  </li>
  <li>encoder-decoder network
    <ul>
      <li>Scene coordinate regression: <a href="https://arxiv.org/pdf/1802.03237.pdf">Full-Frame Scene Coordinate Regression for Image-Based Localization</a></li>
      <li>Single-view depth estimation: <a href="https://arxiv.org/pdf/1704.07813.pdf">Unsupervised Learning of Depth and Ego-Motion from Video</a> <a href="https://github.com/tinghuiz/SfMLearner">Github</a></li>
      <li>Single-view depth estimation: <a href="https://arxiv.org/pdf/1709.06841.pdf">UnDeepVO: Monocular Visual Odometry through Unsupervised Deep Learning</a></li>
      <li>Depth completion: <a href="http://deepcompletion.cs.princeton.edu/paper.pdf">Deep Depth Completion of a Single RGB-D Image</a></li>
      <li><strong>DispNet</strong>: <a href="https://arxiv.org/pdf/1512.02134.pdf">A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation</a></li>
    </ul>
  </li>
  <li>geometry + deep learning
    <ul>
      <li><a href="https://gfx.uvic.ca/pubs/2018/zheng2018eigen/paper.pdf">Eigendecomposition-free Training of Deep Networks with Zero Eigenvalue-based Losses</a></li>
      <li><a href="https://gfx.uvic.ca/pubs/2018/yi2018learning/paper.pdf">Learning to Find Good Correspondences</a></li>
    </ul>
  </li>
  <li>Bayesian Neural Network &amp; MCMC Sampling
    <ul>
      <li><a href="https://www.coursera.org/learn/bayesian-methods-in-machine-learning/lecture/HI8ta/bayesian-neural-networks">Coursera: Bayesian Neural Networks</a></li>
      <li>SGLD: <a href="https://www.ics.uci.edu/~welling/publications/papers/stoclangevin_v6.pdf">Bayesian Learning via Stochastic Gradient Langevin Dynamics</a>: (ICML2011)</li>
      <li>preconditioned SGLD: <a href="http://people.ee.duke.edu/~lcarin/aaai_psgld_final.pdf">Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks</a>: (AAAI 2016)</li>
      <li><a href="https://arxiv.org/pdf/1506.04416.pdf">Bayesian Dark Knowledge</a>: (NIPS2015)</li>
      <li><a href="https://arxiv.org/pdf/1509.05909.pdf">Modelling Uncertainty in Deep Learning for Camera Relocalization</a></li>
      <li><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Brachmann_DSAC_-_Differentiable_CVPR_2017_paper.pdf">DSAC-Differentiable RANSAC for camera localization</a></li>
      <li><a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Li_Learning_Weight_Uncertainty_CVPR_2016_paper.pdf">Learning Weight Uncertainty with Stochastic Gradient MCMC for Shape Classification</a></li>
    </ul>
  </li>
  <li>Optical Flow
    <ul>
      <li><a href="https://arxiv.org/pdf/1504.06852.pdf">FlowNet</a></li>
      <li><a href="https://arxiv.org/pdf/1612.01925.pdf">FlowNet2.0</a> <a href="https://github.com/lmb-freiburg/flownet2">[github]</a></li>
      <li><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yin_GeoNet_Unsupervised_Learning_CVPR_2018_paper.pdf">GeoNet: unsupervide depth, optical flow &amp; pose</a> <a href="https://github.com/yzcjtr/GeoNet">[github]</a></li>
      <li><a href="https://arxiv.org/pdf/1607.07716.pdf">joint optical flow + segmentation</a></li>
      <li><a href="https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/cvpr17_sfm_final.pdf">Unsupervised depth &amp; ego-motion</a> <a href="https://github.com/tinghuiz/SfMLearner">[github]</a> (CVPR 2017 Oral)</li>
      <li><a href="https://arxiv.org/pdf/1709.06750.pdf">SegFlow: optical flow + object segmentation</a> <a href="https://github.com/JingchunCheng/SegFlow">[github]</a></li>
      <li><a href="http://www.cvlibs.net/publications/Behl2017ICCV.pdf">stereo + segmentation + optical flow</a></li>
      <li><a href="https://arxiv.org/pdf/1609.03677.pdf">Unsupervised, depth + stereo</a> (CVPR 2017)</li>
      <li><a href="https://www.robots.ox.ac.uk/~vgg/publications/2015/Pfister15a/pfister15a.pdf">Flowing ConvNets for Human Pose Estimation in Videos</a> Optical flow + human pose</li>
      <li><a href="https://arxiv.org/pdf/1703.10898.pdf">Thin-Slicing Network: A Deep Structured Model for Pose Estimation in Videos</a> Optical flow + human pose</li>
      <li><a href="http://vladlen.info/papers/DCFlow.pdf">DCFlow: Accurate Optical Flow via Direct Cost Volume Processing</a></li>
      <li><a href="https://arxiv.org/pdf/1709.02371.pdf">PWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume</a> <a href="https://github.com/NVlabs/PWC-Net">[github]</a></li>
    </ul>
  </li>
  <li>CRF
    <ul>
      <li><a href="https://arxiv.org/pdf/1411.6387.pdf">Deep Convolutional Neural Fields for Depth Estimation from a Single Imag</a></li>
      <li><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Xu_Multi-Scale_Continuous_CRFs_CVPR_2017_paper.pdf">Multi-Scale Continuous CRFs as Sequential Deep Networks for Monocular Depth Estimation</a></li>
      <li><a href="https://arxiv.org/pdf/1210.5644.pdf">Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials</a></li>
    </ul>
  </li>
</ul>

<h1 id="pnp-algorithm">PnP algorithm</h1>

<blockquote>
  <p>“Perspective-n-Point (PnP) is the problem of estimating the pose of a calibrated camera given a set of n 3D points in the world and their corresponding 2D projections in the image.” from wikipedia</p>
</blockquote>

<h3 id="p3p">P3P</h3>

<p>P3P is the PnP problem in its minimal form. Let P be the camera center, and \(A\), \(B\), \(C\) be the 3D points in world frame. Let \(\|PA\|=X\), \(\|PB\|=Y\), \(\|PC\|=Z\), \(\alpha=\angle BPC\), \(\beta=\angle APC\), \(\gamma=\angle APB\), \(p=2cos\alpha\), \(q=2cos\beta\), \(r=2cos\gamma\), \(\|AB\| = c'\), \(\|BC\| = a'\), \(\|AC\| = b'\), as illustrated in Figure 1.</p>

<p><img src="/images/p3p.png" alt="P3P" />
<strong><center>Figure1. The P3P problem </center></strong></p>

<p>Then the P3P equation system is derived based on the law of cosines (余弦定理):</p>

\[\begin{cases} Y^2 + Z^2 - YZp - a'^2 = 0 \\ Z^2 + X^2 - XZq - b'^2 = 0  \\ X^2 + Y^2 - YXr - c'^2 = 0, \end{cases}\]

<p>where \(X\), \(Y\), \(Z\) are variables to be determined.</p>

<p>To simply the equation system, we let \(X= xZ\), \(Y = yZ\), \(c'= \sqrt{v}Z\), \(a'=\sqrt{av}Z\), \(b'=\sqrt{bv}Z\), then the equation system is turned into</p>

\[\begin{cases} y^2 + 1 - py - av = 0 \\ x^2 + 1 - qx -bv = 0 \\ x^2 + y^2 - xyr - v = 0, \end{cases}\]

<p>where \(x\), \(y\), \(v\) are variables to be determined.</p>

<p>Eliminating \(v = x^2 + y^2 - xyr\), we have</p>

\[\begin{cases} ax^2 + (a-1)y^2 - ar xy + py =1 \\ (b-1)x^2 + b y^2 - brxy + qx = 1.  \end{cases}\]

<p>Essentially, the two quatratic equations define two conics, which may have infinite or at most four intersections. Therefore, <em>the P3P problem has either an infinite number of solutions or at most four physical solutions</em>.</p>

<h3 id="p4p-p5p-pnp">P4P, P5P, PnP</h3>

<p>Using more than 3 points provides redundant data for the solution of pose estimation. The linear algorithm is generally used [1]. The \(n\) points can form \(\frac{n(n-1)}{2}\) triangles and thus derive \(\frac{n(n-1)}{2}\) second degree polynomial equations based on 余弦定理. By eliminating the set of variables into a single one, we still have \(\frac{n(n-1)}{2} - (n-1) = \frac{(n-1)(n-2)}{2}\) fourth degree polynomial equations. When n = 5, we have the homogeneous equation system:</p>

<p><img src="/images/p5p.png" alt="P5P" /></p>

<p>The overdermined equation system can be easily solved by SVD decomposition, where \(t_5\) equals to the right singular vector corresponding to the smallest singular value. The solution can be generalized to cases when \(n&gt;5\).</p>

<p>When \(n=4\), things are a little bit more complicated since the equation system above is underdetermined. We refer the readers to [1] for details.</p>

<h3 id="epnp"><a href="http://icwww.epfl.ch/~lepetit/papers/lepetit_ijcv08.pdf">EPnP</a></h3>

<p>EPnP algorithm is one of the most efficient PnP algorithm which has O(n) complexity. It is composed of three steps:</p>

<p>Step 1. In world coordinate system, find four control points: \(c_j^w (j = 1,...,4)\). One of them is the centroid of the 3D points and the other three are vectors aligned with principle directions of the 3D point set. Then all the 3D points are represented by the <strong><a href="https://en.wikipedia.org/wiki/Barycentric_coordinate_system">barycentric coordinates</a></strong> as a linear combination of the four control points: \(p_i^w = \sum_{j=1}^4 \alpha_{ij} c_j^w (i=1,...,n)\).</p>

<p>Step 2. In camera coordinate system, the equations between 3D points and control points still preserve:  \(p_i^c = \sum_{j=1}^4 \alpha_{ij} c_j^c\). Given the 2D projections of the 3D points \(\{p_i\}i=1,...,n\), we have \(\omega_i \begin{bmatrix} u_i \\v_i \\ 1 \end{bmatrix} = K p_i^c = K \sum_{j=1}^4 \alpha_{ij} c_j^c\). Here, \(\omega_i\) and \(\{c_j^c\}j=1,...,4\) are unknowns. By filling in the camera intrinsic matrix, the equation can be expressed as follow:</p>

\[\omega_i \begin{bmatrix} u_i \\ v_i \\ 1 \end{bmatrix} = \begin{bmatrix} f_u &amp; 0 &amp; u_c \\ 0 &amp; f_v &amp; v_c \\ 0 &amp; 0 &amp; 1 \end{bmatrix}  \begin{bmatrix} \sum_{j=1}^4 \alpha_{ij} x_j^c \\ \sum_{j=1}^4 \alpha_{ij} y_j^c \\ \sum_{j=1}^4 \alpha_{ij} z_j^c \end{bmatrix}.\]

<p>By eliminating \(\omega_i = \sum_{j=1}^4 \alpha_{ij}z_j^c\), we get two linear equations:</p>

\[\begin{cases} \sum_{j=1}^4 \big( \alpha_{ij}f_u x_j^c + \alpha_{ij} (u_c - u_i) z_j^c \big) = 0 \\ \sum_{j=1}^4 \big( \alpha_{ij}f_v y_j^c + \alpha_{ij} (v_c - v_i) z_j^c \big) = 0 \end{cases}.\]

<p>Therefore, each 3D-2D correspondence yields two linear equations with respect to a 12-vector \(x=[x_1^c, y_1^c, z_1^c, ..., x_4^c, y_4^c, z_4^c]^T\). If consider all the correspondences, we generate a linear system of the form</p>

\[Mx = 0.\]

<p>If we have normalized the coordinates of image pixels to \([u_i, v_i]^T\), the equation system becomes</p>

\[\begin{bmatrix} \alpha_{i1} &amp; \alpha_{i2} &amp; \alpha_{i3} &amp; \alpha_{i4} &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; -u_i \alpha_{i1} &amp; -u_i \alpha_{i2} &amp; -u_i \alpha_{i3} &amp; -u_i \alpha_{i4} \\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \alpha_{i1} &amp; \alpha_{i2} &amp; \alpha_{i3} &amp; \alpha_{i4} &amp; -v_i \alpha_{i1} &amp; -v_i \alpha_{i2} &amp; -v_i \alpha_{i3} &amp; -v_i \alpha_{i4} \end{bmatrix} \begin{bmatrix} x_1^c \\ x_2^c \\ x_3^c \\ x_4^c \\ y_1^c \\ y_2^c \\ y_3^c \\ y_4^c \\ z_1^c \\ z_2^c \\ z_3^c \\ z_4^c \end{bmatrix} \\ =   \left(\begin{bmatrix} 1 &amp; 0 &amp; -u_i \\ 0 &amp; 1 &amp; -v_i \end{bmatrix}  \otimes \mathbf{\alpha}_i^T \right)  \mathbf{x} =  \mathbf{0},\]

<p>where \(\otimes\) denotes Kronecker product.</p>

<p>Step 3. The solution of the linear equation system lies in the <strong>null space</strong> or <strong>kernel</strong> of \(M^TM\) and can be expressed as \(x = \sum_{i=1}^N \beta_i v_i\), where \(v_i\) are the right-singular vectors of \(M\) corresponding to the zero singular values. Now the unknowns become \(\{\beta_i\}i=1,...,N\). Based on the analysis in [2], the dimension \(N\) of null-space of \(M^TM\) varies from 1 to 4. The algorithm then considers all the four cases and choose the best set of \(\{\beta_i\}i=1,...,N\) with the miminum reprojection error.</p>

<h3 id="other-pnps-and-insights">Other PnPs and insights</h3>

<ul>
  <li>RPnP: <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6143946">A Robust O(n) Solution to the Perspective-n-Point Problem</a></li>
  <li>OPnP: <a href="http://www2.maths.lth.se/vision/publdb/reports/pdf/zheng-kuang-etal-iiccvi-13.pdf">Revisiting the PnP Problem: A Fast, General and Optimal Solution</a></li>
  <li>REPPnP: <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6909465">Very Fast Solution to the PnP Problem with Algebraic Outlier Rejection</a></li>
  <li>ASPnP: <a href="https://www.jstage.jst.go.jp/article/transinf/E96.D/7/E96.D_1525/_pdf">ASPnP: An Accurate and Scalable Solution to the Perspective-n-Point Problem</a></li>
  <li>The accuracy of solutions to the PnP problem is closely related to the configuration of the 3D point set [RPnP]. Ordinary 3D case, planar case, quasisingular case and “uncentered data” [EPnP].</li>
  <li>Under the assumption of independently and identically distribution (i.i.d.) Gaussian noise, minimizing the reprojection error is statistically optimal in the sense of maximum likelihood estimation (MLE).</li>
  <li>Pose ambiguity
    <ol>
      <li><a href="https://pdfs.semanticscholar.org/85ad/143e7fe0eafe536bb769de3cc5324500dd58.pdf">Robust Pose Estimation from a Planar Target</a></li>
    </ol>
  </li>
</ul>

<h1 id="scene-coordinate-regression-network-score-net">Scene COordinate REgression Network (SCORE-Net)</h1>

<h3 id="network-architecture">Network architecture</h3>

<ul>
  <li>
    <p><a href="http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/en/">Globally and Locally Consistent Image Completion</a></p>
  </li>
  <li>
    <p>Dilated convolution (空洞卷积) Fully convolutional betwork, dilated conv</p>
  </li>
  <li>
    <p><strong><a href="http://statisticsbyjim.com/regression/heteroscedasticity-regression/">Heteroscedasticity</a></strong> 异方差，变量的方差各不相同。Remidial actions for severe heteroscedasticity are necessary.</p>
  </li>
  <li>
    <p><a href="https://stats.stackexchange.com/questions/97832/how-do-you-find-weights-for-weighted-least-squares-regression?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa">Weighted least square</a></p>
  </li>
  <li>
    <p><a href="http://cvrs.whu.edu.cn/downloads/ebooks/Multiple%20View%20Geometry%20in%20Computer%20Vision%20(Second%20Edition).pdf">“Data normalization (or called pre-conditioning in the numerical literature) is an essential step in the DLT algorithm.”</a> from Page 107</p>
  </li>
  <li>
    <p>rank-constrained optimization</p>
  </li>
  <li>
    <p>degenerate cases and the coplanar case</p>
  </li>
</ul>

<h1 id="reference">Reference</h1>
<p>[1] <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=784291">Linear N-Point Camera Pose Determination</a></p>

<p>[2] <a href="http://icwww.epfl.ch/~lepetit/papers/lepetit_ijcv08.pdf">EPnP: An Accurate O(n)Solution to the PnP Problem</a></p>


	<div class="divider"></div>

	<!-- LikeBtn.com BEGIN -->
	<div class="footer">
		<span class="block">&hearts; <i> I appreciate your feedback! &hearts; </i></span>
	</div>
	<div style="text-align: center;">
		<span class="likebtn-wrapper" data-identifier="Image Localization"></span>
		<script>(function (d, e, s) { if (d.getElementById("likebtn_wjs")) return; a = d.createElement(e); m = d.getElementsByTagName(e)[0]; a.async = 1; a.id = "likebtn_wjs"; a.src = s; m.parentNode.insertBefore(a, m) })(document, "script", "//w.likebtn.com/js/w/widget.js");</script>
	</div>
	<!-- LikeBtn.com END -->

</article>





<div class="page-navigation">
	
	<a class="next" href="/deep-learning" title="NEXT: Deep Learning">&lt;&lt;</a>
	<span> &middot; </span>
	
	<a class="home" href="/" title="Back to Homepage">Home</a>
	
	<span> &middot; </span>
	<a class="prev" href="/random-forest" title="PREV: Random Forest">&gt;&gt;</a>
	
</div>
		</main>
                                                      

		<div class="footer">
<!--  <span class="block">Made with &hearts; using <a href="http://jekyllrb.com/">Jekyll</a> &amp; <a href="https://github.com/heiswayi/the-plain" title="Lei's Blog">The Plain</a> &middot; &lt;/&gt; on <a href="https://github.com/zlthinker" title="Hosted on GitHub">GitHub</a></span> -->
  <span class="block">
    &copy; 
    2024 
    Lei ZHOU
    <img src="images/dog.jpg" width="35">
  </span>
</div>


		

			

		

		<!-- hit counter -->
		<br>
		<center>
		<!-- <img src="http://hitwebcounter.com/counter/counter.php?page=6818827&style=0024&nbdigits=5&type=ip&initCount=0" align="top" title="" Alt=""   border="1" height=18px/>     -->
		<script type="text/javascript" src="//counter.websiteout.net/js/5/7/10500/1"></script>
		</center>  

	</body>

</html>
